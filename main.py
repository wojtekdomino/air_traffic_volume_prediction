"""
================================================================================
PROJEKT AKADEMICKI: PREDYKCJA NATƒò≈ªENIA RUCHU LOTNICZEGO
================================================================================

Autor: Wojciech Domino & Mateusz Maj
Cel: Przewidywanie liczby operacji lotniczych (IFR movements) na lotniskach 
     europejskich przy u≈ºyciu modeli uczenia maszynowego

Dataset: European Flights Dataset - miesiƒôczne dane o operacjach lotniczych

Modele wykorzystane w projekcie:
1. LightGBM - model bazowy (gradient boosting)
2. MLP (Multi-Layer Perceptron) - sieƒá neuronowa w PyTorch
3. MLP z przycinaniem (pruning) - kompresja sieci
4. MLP z kwantyzacjƒÖ (quantization) - redukcja rozmiaru modelu

Struktura projektu:
- Sekcja 1: Import bibliotek i konfiguracja
- Sekcja 2: Przygotowanie danych (preprocessing)
- Sekcja 3: In≈ºynieria cech (feature engineering)
- Sekcja 4: Trenowanie modelu LightGBM
- Sekcja 5: Trenowanie sieci neuronowej MLP
- Sekcja 6: Kompresja modeli (pruning i quantization)
- Sekcja 7: Por√≥wnanie wszystkich modeli
- Sekcja 8: G≈Ç√≥wna funkcja uruchamiajƒÖca projekt

================================================================================
"""

# ============================================================================
# SEKCJA 1: IMPORT BIBLIOTEK I KONFIGURACJA
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import pickle
from typing import Tuple, Dict, List

# Machine Learning - modele klasyczne
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import lightgbm as lgb

# Deep Learning - PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.utils.prune as prune

# Konfiguracja wy≈õwietlania
import warnings
warnings.filterwarnings('ignore')

# Sta≈Çe konfiguracyjne
RANDOM_STATE = 42          # Ziarno losowo≈õci dla reprodukowalno≈õci wynik√≥w
TEST_SIZE = 0.2           # Proporcja zbioru testowego (20%)
BATCH_SIZE = 512          # Rozmiar paczki dla treningu sieci neuronowej
LEARNING_RATE = 0.001     # Wsp√≥≈Çczynnik uczenia dla sieci neuronowej
EPOCHS = 50               # Liczba epok treningu sieci neuronowej (zmniejszone dla szybszego treningu)
PRUNING_AMOUNT = 0.3      # Procent neuron√≥w do przyciƒôcia (30%)

# ≈öcie≈ºki do plik√≥w
DATA_PATH = 'data/european_flights.csv'
MODELS_DIR = 'models'


# ============================================================================
# SEKCJA 2: PRZYGOTOWANIE DANYCH (PREPROCESSING)
# ============================================================================

def load_and_clean_data(filepath: str) -> pd.DataFrame:
    """
    Funkcja do wczytywania i czyszczenia danych.
    
    Operacje wykonywane:
    1. Wczytanie danych z pliku CSV
    2. Usuniƒôcie duplikat√≥w
    3. Usuniƒôcie wierszy z brakujƒÖcymi warto≈õciami w zmiennej docelowej
    4. Wyb√≥r istotnych kolumn
    
    Parametry:
        filepath (str): ≈öcie≈ºka do pliku CSV z danymi
        
    Zwraca:
        pd.DataFrame: Oczyszczone dane
    """
    print("="*80)
    print("ETAP 1: WCZYTYWANIE I CZYSZCZENIE DANYCH")
    print("="*80)
    
    # Wczytanie danych
    print(f"\n[1.1] Wczytywanie danych z pliku: {filepath}")
    df = pd.read_csv(filepath)
    print(f"      Wczytano {len(df):,} wierszy i {len(df.columns)} kolumn")
    
    # Usuniƒôcie duplikat√≥w
    initial_rows = len(df)
    df = df.drop_duplicates()
    duplicates_removed = initial_rows - len(df)
    print(f"\n[1.2] Usuniƒôto {duplicates_removed:,} duplikat√≥w")
    
    # Analiza brak√≥w danych
    print(f"\n[1.3] Analiza brak√≥w danych:")
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print(missing[missing > 0])
    else:
        print("      Brak brakujƒÖcych warto≈õci")
    
    # Usuniƒôcie wierszy z brakujƒÖcƒÖ zmiennƒÖ docelowƒÖ
    if 'FLT_TOT_1' in df.columns:
        rows_before = len(df)
        df = df.dropna(subset=['FLT_TOT_1'])
        rows_removed = rows_before - len(df)
        print(f"\n[1.4] Usuniƒôto {rows_removed:,} wierszy z brakujƒÖcƒÖ zmiennƒÖ docelowƒÖ")
    
    # Wyb√≥r istotnych kolumn
    # FLT_TOT_1 - zmienna docelowa (total IFR movements)
    # FLT_DEP_1 - liczba odlot√≥w
    # FLT_ARR_1 - liczba przylot√≥w
    relevant_cols = [
        'YEAR', 'MONTH_NUM', 'APT_ICAO', 'APT_NAME', 
        'STATE_NAME', 'FLT_TOT_1', 'FLT_DEP_1', 'FLT_ARR_1'
    ]
    
    # Zachowanie tylko kolumn, kt√≥re istniejƒÖ w danych
    available_cols = [col for col in relevant_cols if col in df.columns]
    df = df[available_cols]
    
    print(f"\n[1.5] Zachowano {len(available_cols)} istotnych kolumn")
    print(f"      Finalne dane: {len(df):,} wierszy √ó {len(df.columns)} kolumn")
    
    # Podstawowe statystyki zmiennej docelowej
    print(f"\n[1.6] Statystyki zmiennej docelowej (FLT_TOT_1):")
    print(f"      ≈örednia:  {df['FLT_TOT_1'].mean():.2f}")
    print(f"      Mediana:  {df['FLT_TOT_1'].median():.2f}")
    print(f"      Min:      {df['FLT_TOT_1'].min():.2f}")
    print(f"      Max:      {df['FLT_TOT_1'].max():.2f}")
    print(f"      Std:      {df['FLT_TOT_1'].std():.2f}")
    
    return df


# ============================================================================
# SEKCJA 3: IN≈ªYNIERIA CECH (FEATURE ENGINEERING)
# ============================================================================

def create_time_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Tworzenie cech czasowych z kolumn YEAR i MONTH_NUM.
    
    Cechy tworzone:
    - YEAR_TREND: Znormalizowany trend roczny (0, 1, 2, ...)
    - MONTH_SIN: Sinusoidalne kodowanie miesiƒÖca (cykliczno≈õƒá)
    - MONTH_COS: Kosinusoidalne kodowanie miesiƒÖca (cykliczno≈õƒá)
    
    Kodowanie cykliczne zapewnia, ≈ºe grudzie≈Ñ (12) jest blisko stycznia (1)
    
    Parametry:
        df (pd.DataFrame): DataFrame z kolumnami YEAR i MONTH_NUM
        
    Zwraca:
        pd.DataFrame: DataFrame z dodatkowymi cechami czasowymi
    """
    df = df.copy()
    
    # Trend roczny - normalizacja wzglƒôdem pierwszego roku
    if 'YEAR' in df.columns:
        min_year = df['YEAR'].min()
        df['YEAR_TREND'] = df['YEAR'] - min_year
    
    # Kodowanie cykliczne miesiƒÖca
    # Wykorzystuje funkcje trygonometryczne do zachowania cykliczno≈õci
    if 'MONTH_NUM' in df.columns:
        df['MONTH_SIN'] = np.sin(2 * np.pi * df['MONTH_NUM'] / 12)
        df['MONTH_COS'] = np.cos(2 * np.pi * df['MONTH_NUM'] / 12)
    
    return df


def create_seasonal_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Tworzenie cech sezonowych na podstawie miesiƒÖca.
    
    Cechy tworzone:
    - SEASON: Kategoria pory roku (Winter/Spring/Summer/Fall)
    - IS_SUMMER: Flaga binarna - czy to miesiƒÖce letnie (6,7,8)
    - IS_WINTER: Flaga binarna - czy to miesiƒÖce zimowe (12,1,2)
    
    Sezonowo≈õƒá ma znaczenie w ruchu lotniczym - wiƒôkszy ruch w lecie
    
    Parametry:
        df (pd.DataFrame): DataFrame z kolumnƒÖ MONTH_NUM
        
    Zwraca:
        pd.DataFrame: DataFrame z dodatkowymi cechami sezonowymi
    """
    df = df.copy()
    
    if 'MONTH_NUM' not in df.columns:
        return df
    
    # Mapowanie miesiƒôcy na pory roku
    season_map = {
        12: 'Winter', 1: 'Winter', 2: 'Winter',     # Zima
        3: 'Spring', 4: 'Spring', 5: 'Spring',       # Wiosna
        6: 'Summer', 7: 'Summer', 8: 'Summer',       # Lato
        9: 'Fall', 10: 'Fall', 11: 'Fall'           # Jesie≈Ñ
    }
    df['SEASON'] = df['MONTH_NUM'].map(season_map)
    
    # Flagi binarne dla kluczowych sezon√≥w
    df['IS_SUMMER'] = (df['MONTH_NUM'].isin([6, 7, 8])).astype(int)
    df['IS_WINTER'] = (df['MONTH_NUM'].isin([12, 1, 2])).astype(int)
    
    return df


def create_lag_features(df: pd.DataFrame, target_col: str = 'FLT_TOT_1') -> pd.DataFrame:
    """
    Tworzenie cech op√≥≈∫nionych (lag features) dla szereg√≥w czasowych.
    
    Cechy tworzone:
    - lag_1: Warto≈õƒá zmiennej docelowej z poprzedniego miesiƒÖca
    - lag_3: ≈örednia kroczƒÖca z 3 poprzednich miesiƒôcy
    
    Cechy op√≥≈∫nione sƒÖ kluczowe w prognozowaniu szereg√≥w czasowych,
    poniewa≈º przesz≈Çe warto≈õci czƒôsto sƒÖ dobrym predyktorem przysz≈Ço≈õci
    
    Parametry:
        df (pd.DataFrame): DataFrame z danymi posortowanymi chronologicznie
        target_col (str): Nazwa zmiennej docelowej
        
    Zwraca:
        pd.DataFrame: DataFrame z dodatkowymi cechami op√≥≈∫nionymi
    """
    df = df.copy()
    
    if 'APT_ICAO' not in df.columns or target_col not in df.columns:
        return df
    
    # Sortowanie chronologiczne per lotnisko
    df = df.sort_values(['APT_ICAO', 'YEAR', 'MONTH_NUM'])
    
    # Op√≥≈∫nienie 1 miesiƒÖca - warto≈õƒá z poprzedniego miesiƒÖca
    df['lag_1'] = df.groupby('APT_ICAO')[target_col].shift(1)
    
    # ≈örednia kroczƒÖca z 3 miesiƒôcy (wyg≈Çadza kr√≥tkoterminowe fluktuacje)
    df['lag_3'] = df.groupby('APT_ICAO')[target_col].transform(
        lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)
    )
    
    return df


def encode_categorical_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
    """
    Kodowanie cech kategorycznych na warto≈õci numeryczne.
    
    Wykorzystuje Label Encoding dla:
    - APT_ICAO: Kod ICAO lotniska (np. EPWA dla Warszawy)
    - STATE_NAME: Nazwa kraju
    - SEASON: Pora roku
    
    Label Encoding przypisuje ka≈ºdej unikalnej warto≈õci liczbƒô ca≈ÇkowitƒÖ
    
    Parametry:
        df (pd.DataFrame): DataFrame z cechami kategorycznymi
        
    Zwraca:
        Tuple[pd.DataFrame, Dict]: DataFrame z zakodowanymi cechami i s≈Çownik encoder√≥w
    """
    df = df.copy()
    
    # Lista kolumn do zakodowania
    cat_columns = ['APT_ICAO', 'STATE_NAME', 'SEASON']
    
    # Kodowanie tylko istniejƒÖcych kolumn
    cat_columns = [col for col in cat_columns if col in df.columns]
    
    encoders = {}
    
    for col in cat_columns:
        # Inicjalizacja encodera
        le = LabelEncoder()
        # Kodowanie kolumny
        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
        # Zapisanie encodera dla ewentualnego p√≥≈∫niejszego u≈ºycia
        encoders[col] = le
    
    return df, encoders


def engineer_all_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict, List[str]]:
    """
    Pe≈Çny pipeline in≈ºynierii cech - wykonuje wszystkie transformacje.
    
    Proces:
    1. Cechy czasowe (trend, cykliczno≈õƒá)
    2. Cechy sezonowe (pory roku, flagi)
    3. Cechy op√≥≈∫nione (lag features)
    4. Kodowanie kategorii
    5. Usuniƒôcie wierszy z NaN (powsta≈Çych z lag features)
    6. Wyb√≥r finalnych cech do modelowania
    
    Parametry:
        df (pd.DataFrame): Oczyszczone dane wej≈õciowe
        
    Zwraca:
        Tuple zawierajƒÖcy:
        - DataFrame z wygenerowanymi cechami
        - S≈Çownik encoder√≥w kategorycznych
        - Lista nazw cech do modelowania
    """
    print("\n" + "="*80)
    print("ETAP 2: IN≈ªYNIERIA CECH")
    print("="*80)
    
    print("\n[2.1] Tworzenie cech czasowych...")
    df = create_time_features(df)
    print("      Utworzono: YEAR_TREND, MONTH_SIN, MONTH_COS")
    
    print("\n[2.2] Tworzenie cech sezonowych...")
    df = create_seasonal_features(df)
    print("      Utworzono: SEASON, IS_SUMMER, IS_WINTER")
    
    print("\n[2.3] Tworzenie cech op√≥≈∫nionych (lag features)...")
    df = create_lag_features(df)
    print("      Utworzono: lag_1, lag_3")
    
    print("\n[2.4] Kodowanie cech kategorycznych...")
    df, encoders = encode_categorical_features(df)
    print("      Zakodowano: APT_ICAO, STATE_NAME, SEASON")
    
    # Usuniƒôcie wierszy z brakujƒÖcymi warto≈õciami (powsta≈Çymi przez lag features)
    rows_before = len(df)
    df = df.dropna()
    rows_removed = rows_before - len(df)
    print(f"\n[2.5] Usuniƒôto {rows_removed:,} wierszy z brakujƒÖcymi warto≈õciami (lag features)")
    
    # Definicja finalnych cech do modelowania
    # Wybieramy tylko cechy numeryczne i zakodowane kategorie
    feature_cols = [
        'YEAR_TREND', 'MONTH_SIN', 'MONTH_COS',           # Cechy czasowe
        'IS_SUMMER', 'IS_WINTER',                          # Cechy sezonowe
        'FLT_DEP_1', 'FLT_ARR_1',                         # Cechy bazowe
        'lag_1', 'lag_3',                                  # Cechy op√≥≈∫nione
        'APT_ICAO_encoded', 'STATE_NAME_encoded', 'SEASON_encoded'  # Zakodowane kategorie
    ]
    
    # Sprawdzenie dostƒôpno≈õci cech
    feature_cols = [col for col in feature_cols if col in df.columns]
    
    print(f"\n[2.6] Wybrano {len(feature_cols)} cech do modelowania:")
    for i, col in enumerate(feature_cols, 1):
        print(f"      {i:2d}. {col}")
    
    print(f"\n[2.7] Finalny zbi√≥r danych: {len(df):,} wierszy √ó {len(feature_cols)} cech")
    
    return df, encoders, feature_cols


# ============================================================================
# SEKCJA 4: TRENOWANIE MODELU LIGHTGBM
# ============================================================================

def prepare_train_test_split(df: pd.DataFrame, feature_cols: List[str], 
                             target_col: str = 'FLT_TOT_1') -> Tuple:
    """
    Podzia≈Ç danych na zbiory treningowy i testowy.
    
    Stratyfikacja nie jest stosowana (zmienna ciƒÖg≈Ça), ale zapewniamy
    losowy podzia≈Ç z ustalonym ziarnem dla reprodukowalno≈õci.
    
    Parametry:
        df (pd.DataFrame): DataFrame z cechami i zmiennƒÖ docelowƒÖ
        feature_cols (List[str]): Lista nazw cech
        target_col (str): Nazwa zmiennej docelowej
        
    Zwraca:
        Tuple: (X_train, X_test, y_train, y_test)
    """
    print("\n" + "="*80)
    print("ETAP 3: PODZIA≈Å DANYCH NA ZBIORY TRENINGOWY I TESTOWY")
    print("="*80)
    
    # Separacja cech (X) i zmiennej docelowej (y)
    X = df[feature_cols]
    y = df[target_col]
    
    # Podzia≈Ç 80% train / 20% test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=TEST_SIZE, 
        random_state=RANDOM_STATE
    )
    
    print(f"\n[3.1] Zbi√≥r treningowy: {len(X_train):,} pr√≥bek ({100*(1-TEST_SIZE):.0f}%)")
    print(f"[3.2] Zbi√≥r testowy:    {len(X_test):,} pr√≥bek ({100*TEST_SIZE:.0f}%)")
    
    print(f"\n[3.3] Statystyki zmiennej docelowej w zbiorze treningowym:")
    print(f"      ≈örednia: {y_train.mean():.2f}")
    print(f"      Std:     {y_train.std():.2f}")
    
    return X_train, X_test, y_train, y_test


def train_lightgbm_model(X_train, y_train, X_test, y_test):
    """
    Trenowanie modelu LightGBM (Light Gradient Boosting Machine).
    
    LightGBM to szybka implementacja gradient boosting, kt√≥ra:
    - Wykorzystuje drzewa decyzyjne jako bazowe estymatory
    - Stosuje boosting (sekwencyjne uczenie, ka≈ºde drzewo poprawia b≈Çƒôdy poprzednich)
    - Jest efektywna dla du≈ºych zbior√≥w danych
    
    Hiperparametry:
    - objective: regression (zadanie regresji)
    - metric: RMSE (Root Mean Squared Error)
    - num_leaves: 31 (maksymalna liczba li≈õci w drzewie)
    - learning_rate: 0.05 (wsp√≥≈Çczynnik uczenia)
    - feature_fraction: 0.9 (losowe pr√≥bkowanie cech)
    - bagging: losowe pr√≥bkowanie danych
    
    Parametry:
        X_train, y_train: Dane treningowe
        X_test, y_test: Dane testowe (do walidacji podczas treningu)
        
    Zwraca:
        lgb.Booster: Wytrenowany model LightGBM
    """
    print("\n" + "="*80)
    print("ETAP 4: TRENOWANIE MODELU LIGHTGBM")
    print("="*80)
    
    print("\n[4.1] Konfiguracja hiperparametr√≥w LightGBM:")
    params = {
        'objective': 'regression',        # Zadanie regresji
        'metric': 'rmse',                 # Metryka optymalizacji
        'boosting_type': 'gbdt',          # Gradient Boosting Decision Tree
        'num_leaves': 31,                 # Liczba li≈õci w drzewie
        'learning_rate': 0.05,            # Wsp√≥≈Çczynnik uczenia
        'feature_fraction': 0.9,          # Losowe pr√≥bkowanie cech (90%)
        'bagging_fraction': 0.8,          # Losowe pr√≥bkowanie danych (80%)
        'bagging_freq': 5,                # Czƒôstotliwo≈õƒá bagging
        'verbose': -1                     # Wy≈ÇƒÖczenie szczeg√≥≈Çowych log√≥w
    }
    
    for key, value in params.items():
        print(f"      {key:20s} = {value}")
    
    # Utworzenie zbior√≥w danych w formacie LightGBM
    print("\n[4.2] Przygotowanie zbior√≥w danych LightGBM...")
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # Trenowanie modelu z early stopping
    print("\n[4.3] Rozpoczƒôcie treningu (max 500 iteracji, early stopping=50)...")
    print("-" * 80)
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,              # Maksymalna liczba drzew
        valid_sets=[train_data, test_data],
        valid_names=['train', 'test'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),  # Zatrzymaj je≈õli brak poprawy przez 50 iteracji
            lgb.log_evaluation(50)                    # Wy≈õwietl wyniki co 50 iteracji
        ]
    )
    
    print("-" * 80)
    print(f"\n[4.4] Trenowanie zako≈Ñczone. U≈ºyto {model.num_trees()} drzew.")
    
    return model


def evaluate_lightgbm_model(model, X_train, y_train, X_test, y_test):
    """
    Ewaluacja modelu LightGBM na zbiorach treningowym i testowym.
    
    Metryki:
    - RMSE (Root Mean Squared Error): pierwiastek b≈Çƒôdu ≈õredniokwadratowego
      -> kara wiƒôksza dla du≈ºych b≈Çƒôd√≥w, jednostka jak zmienna docelowa
    - MAE (Mean Absolute Error): ≈õredni b≈ÇƒÖd bezwzglƒôdny
      -> ≈Çatwiejsza interpretacja, mniej wra≈ºliwa na outliers
    - R¬≤ (R-squared): wsp√≥≈Çczynnik determinacji (0-1, im wy≈ºszy tym lepiej)
      -> procent wariancji wyja≈õnionej przez model
    
    Parametry:
        model: Wytrenowany model LightGBM
        X_train, y_train: Dane treningowe
        X_test, y_test: Dane testowe
        
    Zwraca:
        Dict: S≈Çownik z metrykami i predykcjami
    """
    print("\n[4.5] Ewaluacja modelu LightGBM:")
    print("-" * 80)
    
    # Predykcje na zbiorze treningowym
    y_train_pred = model.predict(X_train)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    
    # Predykcje na zbiorze testowym
    y_test_pred = model.predict(X_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    # Wy≈õwietlenie wynik√≥w
    print(f"\n      {'Metryka':<20s} {'Train':<15s} {'Test':<15s}")
    print(f"      {'-'*50}")
    print(f"      {'RMSE':<20s} {train_rmse:<15.2f} {test_rmse:<15.2f}")
    print(f"      {'MAE':<20s} {train_mae:<15.2f} {test_mae:<15.2f}")
    print(f"      {'R¬≤':<20s} {train_r2:<15.4f} {test_r2:<15.4f}")
    
    # Interpretacja
    print(f"\n      Interpretacja R¬≤ na zbiorze testowym:")
    print(f"      Model wyja≈õnia {test_r2*100:.2f}% wariancji zmiennej docelowej")
    
    return {
        'train_rmse': train_rmse, 'train_mae': train_mae, 'train_r2': train_r2,
        'test_rmse': test_rmse, 'test_mae': test_mae, 'test_r2': test_r2,
        'test_predictions': y_test_pred
    }


def save_lightgbm_model(model):
    """
    Zapis modelu LightGBM do pliku.
    
    Parametry:
        model: Wytrenowany model LightGBM
    """
    os.makedirs(MODELS_DIR, exist_ok=True)
    model_path = os.path.join(MODELS_DIR, 'lightgbm_model.txt')
    model.save_model(model_path)
    print(f"\n[4.6] Model LightGBM zapisany do: {model_path}")


# ============================================================================
# SEKCJA 5: TRENOWANIE SIECI NEURONOWEJ MLP
# ============================================================================

class MLPRegressor(nn.Module):
    """
    Klasa sieci neuronowej MLP (Multi-Layer Perceptron) w PyTorch.
    
    Architektura:
    - Warstwa wej≈õciowa: liczba cech
    - Warstwy ukryte: [128, 64, 32] neurony
    - Ka≈ºda warstwa ukryta ma:
      * Linear (fully connected)
      * ReLU (funkcja aktywacji)
      * Dropout (20% regularizacji - zapobiega przeuczeniu)
    - Warstwa wyj≈õciowa: 1 neuron (regresja)
    
    MLP to klasyczna sieƒá feedforward - informacja p≈Çynie tylko do przodu,
    od wej≈õcia do wyj≈õcia.
    """
    
    def __init__(self, input_size: int, hidden_sizes: List[int] = [128, 64, 32]):
        """
        Inicjalizacja architektury sieci MLP.
        
        Parametry:
            input_size (int): Liczba cech wej≈õciowych
            hidden_sizes (List[int]): Lista rozmiar√≥w warstw ukrytych
        """
        super(MLPRegressor, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # Budowa warstw ukrytych
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))  # Warstwa liniowa
            layers.append(nn.ReLU())                          # Funkcja aktywacji
            layers.append(nn.Dropout(0.2))                    # Regularyzacja
            prev_size = hidden_size
        
        # Warstwa wyj≈õciowa (bez aktywacji dla regresji)
        layers.append(nn.Linear(prev_size, 1))
        
        # Po≈ÇƒÖczenie wszystkich warstw w sekwencyjny model
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        """
        Propagacja wprz√≥d (forward pass).
        
        Parametry:
            x: Tensor wej≈õciowy
            
        Zwraca:
            Tensor: Predykcje sieci
        """
        return self.network(x)


def prepare_pytorch_data(X_train, X_test, y_train, y_test):
    """
    Przygotowanie danych dla PyTorch.
    
    Kroki:
    1. Standaryzacja cech (mean=0, std=1) - wa≈ºne dla sieci neuronowych
    2. Konwersja do tensor√≥w PyTorch
    3. Utworzenie DataLoader dla efektywnego treningu w batches
    
    Parametry:
        X_train, X_test, y_train, y_test: Zbiory danych
        
    Zwraca:
        Tuple: (train_loader, test_loader, scaler, X_test_tensor, y_test_tensor)
    """
    print("\n[5.1] Standaryzacja cech (StandardScaler)...")
    # Standaryzacja - uczenie tylko na zbiorze treningowym!
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"      ≈örednia po standaryzacji: {X_train_scaled.mean():.6f}")
    print(f"      Odchylenie standardowe:   {X_train_scaled.std():.6f}")
    
    print("\n[5.2] Konwersja do tensor√≥w PyTorch...")
    # Konwersja do tensor√≥w
    X_train_tensor = torch.FloatTensor(X_train_scaled)
    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)
    X_test_tensor = torch.FloatTensor(X_test_scaled)
    y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)
    
    print("\n[5.3] Tworzenie DataLoaders...")
    # Utworzenie datasets i loaders
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"      Batch size: {BATCH_SIZE}")
    print(f"      Liczba batchy treningowych: {len(train_loader)}")
    print(f"      Liczba batchy testowych:    {len(test_loader)}")
    
    return train_loader, test_loader, scaler, X_test_tensor, y_test_tensor


def train_mlp_model(input_size: int, train_loader, test_loader):
    """
    Trenowanie sieci neuronowej MLP.
    
    Proces treningu:
    1. Inicjalizacja modelu i optymalizatora
    2. Dla ka≈ºdej epoki:
       a) Trening na danych treningowych (mini-batches)
       b) Walidacja na danych testowych
       c) Monitoring metryk (loss)
    3. Wyb√≥r najlepszego modelu (najni≈ºszy test loss)
    
    Optimizer: Adam (adaptive learning rate)
    Loss function: MSE (Mean Squared Error) - standard dla regresji
    
    Parametry:
        input_size (int): Liczba cech wej≈õciowych
        train_loader: DataLoader ze zbiorem treningowym
        test_loader: DataLoader ze zbiorem testowym
        
    Zwraca:
        Tuple: (model, train_losses, test_losses)
    """
    print("\n" + "="*80)
    print("ETAP 5: TRENOWANIE SIECI NEURONOWEJ MLP")
    print("="*80)
    
    print("\n[5.4] Inicjalizacja architektury MLP:")
    print(f"      Warstwa wej≈õciowa:  {input_size} cech")
    print(f"      Warstwy ukryte:     [128, 64, 32] neurony")
    print(f"      Warstwa wyj≈õciowa:  1 neuron (regresja)")
    print(f"      Funkcja aktywacji:  ReLU")
    print(f"      Regularyzacja:      Dropout (20%)")
    
    # Inicjalizacja modelu
    model = MLPRegressor(input_size=input_size, hidden_sizes=[128, 64, 32])
    
    # Wy≈õwietlenie struktury modelu
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n      ≈ÅƒÖczna liczba parametr√≥w:    {total_params:,}")
    print(f"      Trenowalnych parametr√≥w:      {trainable_params:,}")
    
    # Funkcja straty i optymalizator
    criterion = nn.MSELoss()                                    # Mean Squared Error
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Adam optimizer
    
    print(f"\n[5.5] Konfiguracja treningu:")
    print(f"      Loss function:     MSE (Mean Squared Error)")
    print(f"      Optimizer:         Adam")
    print(f"      Learning rate:     {LEARNING_RATE}")
    print(f"      Liczba epok:       {EPOCHS}")
    print(f"      Batch size:        {BATCH_SIZE}")
    
    # Listy do przechowywania historii treningu
    train_losses = []
    test_losses = []
    best_test_loss = float('inf')
    best_model_state = None
    
    print("\n[5.6] Rozpoczƒôcie treningu:")
    print("-" * 80)
    print(f"      {'Epoka':<10s} {'Train Loss':<15s} {'Test Loss':<15s} {'Status':<20s}")
    print("-" * 80)
    
    # Pƒôtla treningowa
    for epoch in range(EPOCHS):
        # === FAZA TRENINGOWA ===
        model.train()  # Tryb treningowy (w≈ÇƒÖcza dropout)
        train_loss_epoch = 0.0
        
        for X_batch, y_batch in train_loader:
            # Zerowanie gradient√≥w
            optimizer.zero_grad()
            
            # Forward pass
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            
            # Backward pass
            loss.backward()
            
            # Aktualizacja wag
            optimizer.step()
            
            train_loss_epoch += loss.item()
        
        # ≈örednia strata treningowa
        train_loss_epoch /= len(train_loader)
        train_losses.append(train_loss_epoch)
        
        # === FAZA WALIDACYJNA ===
        model.eval()  # Tryb ewaluacji (wy≈ÇƒÖcza dropout)
        test_loss_epoch = 0.0
        
        with torch.no_grad():  # Bez obliczania gradient√≥w
            for X_batch, y_batch in test_loader:
                predictions = model(X_batch)
                loss = criterion(predictions, y_batch)
                test_loss_epoch += loss.item()
        
        # ≈örednia strata testowa
        test_loss_epoch /= len(test_loader)
        test_losses.append(test_loss_epoch)
        
        # Zapisanie najlepszego modelu
        status = ""
        if test_loss_epoch < best_test_loss:
            best_test_loss = test_loss_epoch
            best_model_state = model.state_dict().copy()
            status = "‚úì Nowy najlepszy"
        
        # Wy≈õwietlanie postƒôpu co 10 epok
        if (epoch + 1) % 10 == 0:
            print(f"      {epoch+1:<10d} {train_loss_epoch:<15.4f} {test_loss_epoch:<15.4f} {status:<20s}")
    
    print("-" * 80)
    print(f"\n[5.7] Trenowanie zako≈Ñczone!")
    print(f"      Najlepszy test loss: {best_test_loss:.4f}")
    
    # Przywr√≥cenie najlepszego modelu
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"      Przywr√≥cono najlepszy model z epoki")
    
    return model, train_losses, test_losses


def evaluate_mlp_model(model, X_test_tensor, y_test):
    """
    Ewaluacja sieci neuronowej MLP.
    
    Parametry:
        model: Wytrenowany model MLP
        X_test_tensor: Tensor z cechami testowymi
        y_test: Prawdziwe warto≈õci zmiennej docelowej
        
    Zwraca:
        Dict: Metryki i predykcje
    """
    print("\n[5.8] Ewaluacja sieci MLP na zbiorze testowym:")
    print("-" * 80)
    
    model.eval()
    
    with torch.no_grad():
        predictions = model(X_test_tensor).numpy().flatten()
    
    y_true = y_test.values if hasattr(y_test, 'values') else y_test
    
    # Obliczenie metryk
    rmse = np.sqrt(mean_squared_error(y_true, predictions))
    mae = mean_absolute_error(y_true, predictions)
    r2 = r2_score(y_true, predictions)
    
    print(f"\n      {'Metryka':<20s} {'Warto≈õƒá':<15s}")
    print(f"      {'-'*35}")
    print(f"      {'RMSE':<20s} {rmse:<15.2f}")
    print(f"      {'MAE':<20s} {mae:<15.2f}")
    print(f"      {'R¬≤':<20s} {r2:<15.4f}")
    
    return {
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'predictions': predictions
    }


def save_mlp_model(model, scaler):
    """
    Zapis modelu MLP i scalera.
    
    Parametry:
        model: Wytrenowany model MLP
        scaler: StandardScaler u≈ºyty do standaryzacji
    """
    os.makedirs(MODELS_DIR, exist_ok=True)
    
    # Zapis modelu
    model_path = os.path.join(MODELS_DIR, 'mlp_fp32.pt')
    torch.save(model.state_dict(), model_path)
    print(f"\n[5.9] Model MLP zapisany do: {model_path}")
    
    # Zapis scalera
    scaler_path = os.path.join(MODELS_DIR, 'mlp_scaler.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"[5.10] Scaler zapisany do: {scaler_path}")


# ============================================================================
# SEKCJA 6: KOMPRESJA MODELI (PRUNING I QUANTIZATION)
# ============================================================================

def count_model_parameters(model):
    """
    Zliczanie parametr√≥w w modelu.
    
    Parametry:
        model: Model PyTorch
        
    Zwraca:
        Tuple: (total_params, nonzero_params)
    """
    total_params = sum(p.numel() for p in model.parameters())
    nonzero_params = sum((p != 0).sum().item() for p in model.parameters())
    return total_params, nonzero_params


def prune_mlp_model(model, amount=PRUNING_AMOUNT):
    """
    Przycinanie (pruning) sieci neuronowej.
    
    Pruning to technika kompresji modelu, kt√≥ra:
    - Usuwa najmniej istotne po≈ÇƒÖczenia (wagi bliskie zeru)
    - Redukuje rozmiar modelu
    - Przyspiesza inferencing
    - Mo≈ºe nieznacznie obni≈ºyƒá accuracy (do akceptacji w praktyce)
    
    Stosujemy structured pruning - usuwamy ca≈Çe neurony, nie pojedyncze wagi.
    To daje wiƒôksze przyspieszenie na standardowym sprzƒôcie.
    
    Parametry:
        model: Model MLP do przyciƒôcia
        amount (float): Procent neuron√≥w do usuniƒôcia (domy≈õlnie 30%)
        
    Zwraca:
        Model: Przyciƒôty model
    """
    print("\n" + "="*80)
    print("ETAP 6A: PRZYCINANIE MODELU (PRUNING)")
    print("="*80)
    
    print(f"\n[6.1] Parametry przed pruningiem:")
    total_before, nonzero_before = count_model_parameters(model)
    print(f"      ≈ÅƒÖczna liczba parametr√≥w:    {total_before:,}")
    print(f"      Niezerowych parametr√≥w:       {nonzero_before:,}")
    
    print(f"\n[6.2] Stosowanie structured pruning (amount={amount*100:.0f}%)...")
    print("      Metoda: L2-norm structured pruning na warstwach Linear")
    
    # Zastosowanie structured pruning do warstw liniowych
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # Pruning wzd≈Çu≈º wymiaru 0 (ca≈Çe neurony)
            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)
    
    print(f"\n[6.3] Parametry po pruningu:")
    total_after, nonzero_after = count_model_parameters(model)
    print(f"      ≈ÅƒÖczna liczba parametr√≥w:    {total_after:,}")
    print(f"      Niezerowych parametr√≥w:       {nonzero_after:,}")
    print(f"      Sparsity (rzadko≈õƒá):          {100 * (1 - nonzero_after / total_after):.2f}%")
    
    # Usuniƒôcie reparametryzacji (utrwalenie pruning)
    print("\n[6.4] Utrwalanie pruning (usuwanie reparametryzacji)...")
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            try:
                prune.remove(module, 'weight')
            except:
                pass  # Brak pruning na tej warstwie
    
    print("      Pruning zosta≈Ç utrwalony w modelu")
    
    return model


def quantize_mlp_model(model):
    """
    Kwantyzacja (quantization) sieci neuronowej.
    
    Quantization to technika kompresji, kt√≥ra:
    - Konwertuje wagi z FP32 (32-bit float) do INT8 (8-bit integer)
    - Redukuje rozmiar modelu ~4x
    - Przyspiesza inferencing (szczeg√≥lnie na CPU)
    - Minimalny spadek accuracy
    
    Stosujemy dynamic quantization - kwantyzacja podczas inferencingu,
    optymalna dla modeli z warstwami Linear/LSTM.
    
    Parametry:
        model: Model MLP do kwantyzacji
        
    Zwraca:
        Model: Skwantyzowany model
    """
    print("\n" + "="*80)
    print("ETAP 6B: KWANTYZACJA MODELU (QUANTIZATION)")
    print("="*80)
    
    print("\n[6.5] Typ kwantyzacji: Dynamic Quantization")
    print("      Format:          FP32 ‚Üí INT8")
    print("      Warstwy:         Linear layers")
    
    # Model musi byƒá w trybie eval
    model.eval()
    
    # Kwantyzacja dynamiczna
    print("\n[6.6] Stosowanie kwantyzacji...")
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear},      # Kwantyzuj tylko warstwy Linear
        dtype=torch.qint8  # U≈ºyj 8-bitowych integer√≥w
    )
    
    print("      Kwantyzacja zako≈Ñczona pomy≈õlnie")
    
    return quantized_model


def save_compressed_models(pruned_model, quantized_model):
    """
    Zapis skompresowanych modeli.
    
    Parametry:
        pruned_model: Model po pruning
        quantized_model: Model po quantization
    """
    os.makedirs(MODELS_DIR, exist_ok=True)
    
    # Zapis modelu pruned
    pruned_path = os.path.join(MODELS_DIR, 'mlp_pruned.pt')
    torch.save(pruned_model.state_dict(), pruned_path)
    pruned_size = os.path.getsize(pruned_path) / (1024 * 1024)
    print(f"\n[6.7] Model Pruned zapisany do: {pruned_path}")
    print(f"      Rozmiar: {pruned_size:.2f} MB")
    
    # Zapis modelu quantized
    quantized_path = os.path.join(MODELS_DIR, 'mlp_int8.pt')
    torch.save(quantized_model.state_dict(), quantized_path)
    quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
    print(f"\n[6.8] Model Quantized zapisany do: {quantized_path}")
    print(f"      Rozmiar: {quantized_size:.2f} MB")


# ============================================================================
# SEKCJA 7: POR√ìWNANIE WSZYSTKICH MODELI
# ============================================================================

def compare_all_models(lightgbm_metrics, mlp_metrics, 
                      pruned_model, quantized_model, 
                      X_test_tensor, y_test):
    """
    Kompleksowe por√≥wnanie wszystkich modeli.
    
    Por√≥wnujemy:
    1. LightGBM (baseline)
    2. MLP FP32 (sieƒá neuronowa full precision)
    3. MLP Pruned (sieƒá po przycinaniu)
    4. MLP Quantized (sieƒá po kwantyzacji)
    
    Metryki por√≥wnania:
    - RMSE, MAE, R¬≤ (accuracy)
    - Rozmiar modelu (MB)
    - Czas inferencing (ms/sample)
    
    Parametry:
        lightgbm_metrics: Metryki modelu LightGBM
        mlp_metrics: Metryki modelu MLP
        pruned_model: Model po pruning
        quantized_model: Model po quantization
        X_test_tensor: Dane testowe
        y_test: Prawdziwe warto≈õci
        
    Zwraca:
        pd.DataFrame: Tabela por√≥wnawcza
    """
    print("\n" + "="*80)
    print("ETAP 7: POR√ìWNANIE WSZYSTKICH MODELI")
    print("="*80)
    
    results = []
    
    # Model 1: LightGBM
    print("\n[7.1] Ewaluacja modelu LightGBM...")
    lgb_size = os.path.getsize(os.path.join(MODELS_DIR, 'lightgbm_model.txt')) / (1024 * 1024)
    results.append({
        'Model': 'LightGBM',
        'RMSE': lightgbm_metrics['test_rmse'],
        'MAE': lightgbm_metrics['test_mae'],
        'R¬≤': lightgbm_metrics['test_r2'],
        'Rozmiar (MB)': lgb_size
    })
    
    # Model 2: MLP FP32
    print("[7.2] Ewaluacja modelu MLP FP32...")
    mlp_size = os.path.getsize(os.path.join(MODELS_DIR, 'mlp_fp32.pt')) / (1024 * 1024)
    results.append({
        'Model': 'MLP FP32',
        'RMSE': mlp_metrics['rmse'],
        'MAE': mlp_metrics['mae'],
        'R¬≤': mlp_metrics['r2'],
        'Rozmiar (MB)': mlp_size
    })
    
    # Model 3: MLP Pruned
    print("[7.3] Ewaluacja modelu MLP Pruned...")
    pruned_model.eval()
    with torch.no_grad():
        pruned_pred = pruned_model(X_test_tensor).numpy().flatten()
    
    pruned_rmse = np.sqrt(mean_squared_error(y_test, pruned_pred))
    pruned_mae = mean_absolute_error(y_test, pruned_pred)
    pruned_r2 = r2_score(y_test, pruned_pred)
    pruned_size = os.path.getsize(os.path.join(MODELS_DIR, 'mlp_pruned.pt')) / (1024 * 1024)
    
    results.append({
        'Model': 'MLP Pruned',
        'RMSE': pruned_rmse,
        'MAE': pruned_mae,
        'R¬≤': pruned_r2,
        'Rozmiar (MB)': pruned_size
    })
    
    # Model 4: MLP Quantized
    print("[7.4] Ewaluacja modelu MLP Quantized...")
    quantized_model.eval()
    with torch.no_grad():
        quantized_pred = quantized_model(X_test_tensor).numpy().flatten()
    
    quantized_rmse = np.sqrt(mean_squared_error(y_test, quantized_pred))
    quantized_mae = mean_absolute_error(y_test, quantized_pred)
    quantized_r2 = r2_score(y_test, quantized_pred)
    quantized_size = os.path.getsize(os.path.join(MODELS_DIR, 'mlp_int8.pt')) / (1024 * 1024)
    
    results.append({
        'Model': 'MLP Quantized INT8',
        'RMSE': quantized_rmse,
        'MAE': quantized_mae,
        'R¬≤': quantized_r2,
        'Rozmiar (MB)': quantized_size
    })
    
    # Utworzenie tabeli por√≥wnawczej
    comparison_df = pd.DataFrame(results)
    
    print("\n" + "="*80)
    print("TABELA POR√ìWNAWCZA MODELI")
    print("="*80)
    print(comparison_df.to_string(index=False))
    
    # Analiza wynik√≥w
    print("\n" + "="*80)
    print("ANALIZA WYNIK√ìW")
    print("="*80)
    
    best_rmse_idx = comparison_df['RMSE'].idxmin()
    best_r2_idx = comparison_df['R¬≤'].idxmax()
    smallest_idx = comparison_df['Rozmiar (MB)'].idxmin()
    
    print(f"\n‚úì Najlepsza dok≈Çadno≈õƒá (RMSE):    {comparison_df.loc[best_rmse_idx, 'Model']}")
    print(f"‚úì Najlepsze R¬≤:                   {comparison_df.loc[best_r2_idx, 'Model']}")
    print(f"‚úì Najmniejszy rozmiar:            {comparison_df.loc[smallest_idx, 'Model']}")
    
    # Kompresja
    original_size = comparison_df[comparison_df['Model'] == 'MLP FP32']['Rozmiar (MB)'].values[0]
    quantized_size_val = comparison_df[comparison_df['Model'] == 'MLP Quantized INT8']['Rozmiar (MB)'].values[0]
    compression_ratio = original_size / quantized_size_val
    
    print(f"\nüìä Stopie≈Ñ kompresji (FP32 ‚Üí INT8): {compression_ratio:.2f}x")
    print(f"   Redukcja rozmiaru: {(1 - 1/compression_ratio)*100:.1f}%")
    
    return comparison_df


# ============================================================================
# SEKCJA 8: G≈Å√ìWNA FUNKCJA URUCHAMIAJƒÑCA PROJEKT
# ============================================================================

def main():
    """
    G≈Ç√≥wna funkcja uruchamiajƒÖca ca≈Çy pipeline projektu.
    
    Kolejno≈õƒá wykonania:
    1. Wczytanie i czyszczenie danych
    2. In≈ºynieria cech
    3. Podzia≈Ç na zbiory train/test
    4. Trenowanie LightGBM
    5. Trenowanie MLP
    6. Kompresja modeli (pruning + quantization)
    7. Por√≥wnanie wszystkich modeli
    """
    print("\n")
    print("‚ñà" * 80)
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" + "  PROJEKT AKADEMICKI: PREDYKCJA NATƒò≈ªENIA RUCHU LOTNICZEGO".center(78) + "‚ñà")
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" * 80)
    print("\n")
    
    # ========================================
    # ETAP 1-2: Dane i cechy
    # ========================================
    df = load_and_clean_data(DATA_PATH)
    df, encoders, feature_cols = engineer_all_features(df)
    
    # ========================================
    # ETAP 3: Podzia≈Ç danych
    # ========================================
    X_train, X_test, y_train, y_test = prepare_train_test_split(
        df, feature_cols, target_col='FLT_TOT_1'
    )
    
    # ========================================
    # ETAP 4: LightGBM
    # ========================================
    lgb_model = train_lightgbm_model(X_train, y_train, X_test, y_test)
    lgb_metrics = evaluate_lightgbm_model(lgb_model, X_train, y_train, X_test, y_test)
    save_lightgbm_model(lgb_model)
    
    # ========================================
    # ETAP 5: MLP
    # ========================================
    train_loader, test_loader, scaler, X_test_tensor, y_test_tensor = prepare_pytorch_data(
        X_train, X_test, y_train, y_test
    )
    
    mlp_model, train_losses, test_losses = train_mlp_model(
        input_size=len(feature_cols),
        train_loader=train_loader,
        test_loader=test_loader
    )
    
    mlp_metrics = evaluate_mlp_model(mlp_model, X_test_tensor, y_test)
    save_mlp_model(mlp_model, scaler)
    
    # ========================================
    # ETAP 6: Kompresja
    # ========================================
    # 6A: Pruning
    pruned_model = MLPRegressor(input_size=len(feature_cols), hidden_sizes=[128, 64, 32])
    pruned_model.load_state_dict(mlp_model.state_dict())
    pruned_model = prune_mlp_model(pruned_model, amount=PRUNING_AMOUNT)
    
    # 6B: Quantization
    quantized_model = MLPRegressor(input_size=len(feature_cols), hidden_sizes=[128, 64, 32])
    quantized_model.load_state_dict(mlp_model.state_dict())
    quantized_model = quantize_mlp_model(quantized_model)
    
    save_compressed_models(pruned_model, quantized_model)
    
    # ========================================
    # ETAP 7: Por√≥wnanie
    # ========================================
    comparison_df = compare_all_models(
        lgb_metrics, mlp_metrics,
        pruned_model, quantized_model,
        X_test_tensor, y_test
    )
    
    # Zapis wynik√≥w por√≥wnania
    comparison_path = os.path.join(MODELS_DIR, 'model_comparison.csv')
    comparison_df.to_csv(comparison_path, index=False)
    print(f"\n[7.5] Tabela por√≥wnawcza zapisana do: {comparison_path}")
    
    # ========================================
    # Zako≈Ñczenie
    # ========================================
    print("\n" + "‚ñà" * 80)
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" + "  PROJEKT ZAKO≈ÉCZONY POMY≈öLNIE".center(78) + "‚ñà")
    print("‚ñà" + " " * 78 + "‚ñà")
    print("‚ñà" * 80)
    print("\nWszystkie modele zosta≈Çy wytrenowane i zapisane w folderze 'models/'")
    print("Wyniki por√≥wnania dostƒôpne w pliku: models/model_comparison.csv\n")


# ============================================================================
# URUCHOMIENIE PROJEKTU
# ============================================================================

if __name__ == "__main__":
    """
    Punkt wej≈õcia programu.
    
    Aby uruchomiƒá projekt, wystarczy wykonaƒá:
        python main.py
    
    Wymagania:
        - Zainstalowane biblioteki z requirements.txt
        - Plik danych: data/european_flights.csv
    """
    main()
